{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random  \n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  # BLEU score: Measures how similar two sentences are.\n",
    "from rouge_score import rouge_scorer  # ROUGE score: Used a lot in text summarization models.\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Cosine similarity: Helps check how close word embeddings are.\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "\n",
    "import time  \n",
    "\n",
    "# Making sure we have access to the WordNet dataset before using it.\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length: int, depth: int):\n",
    "    \"\"\"\n",
    "    Generates positional encodings to provide word order information in a sequence.\n",
    "    Used in Transformer models to replace recurrence.\n",
    "    \"\"\"\n",
    "\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:,np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis,:]/depth\n",
    "    \n",
    "    angle_rates= 1/ (10000**depths)\n",
    "    angle_rads = positions* angle_rates\n",
    "    pos_encoding= np.concatenate([np.sin(angle_rads), np.cos(angle_rads)],axis=-1)\n",
    "    return tf.cast(pos_encoding,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize positional encodings.\n",
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "\n",
    "# Print the shape of the positional encoding (should be [2048, 512]).\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Visual representation of positional encoding (heatmap).\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c07e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding /= tf.norm(pos_encoding,axis=-1,keepdims=True)\n",
    "p = pos_encoding[1000]\n",
    "dots= tf.einsum('pd,d->p',pos_encoding,p).numpy()\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(dots)\n",
    "plt.ylim([0,1])\n",
    "plt.plot([950,950,float('nan'),1050,1050],[ 0,1,float('nan'),0,1], color='k',label='Zoom')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(range(len(dots)),dots)\n",
    "plt.xlim([950,1050])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size: int, d_model: int, embedding: tf.keras.layers.Embedding = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # If no custom embedding is provided, initialize a trainable embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) if embedding is None else embedding\n",
    "        # Generate positional encodings once (for a max sequence length of 2048)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        # Ensure that the mask from the embedding layer is returned\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        seq_len = tf.shape(x)[1]  # Get the actual sequence length of input\n",
    "        x = self.embedding(x)  # Convert token indices to embeddings\n",
    "\n",
    "        # Removed unnecessary `length = tf.shape(x)[:]` (had no effect)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # âœ… Scale embeddings for stability\n",
    "        \n",
    "        # Corrected positional encoding broadcasting:\n",
    "        # Before: self.pos_encoding[tf.newaxis, :seq_len, :]\n",
    "        # After: self.pos_encoding[:seq_len, :]\n",
    "        # - Removed `tf.newaxis` to avoid unnecessary expansion\n",
    "        # - Ensures positional encoding correctly aligns with input tensor\n",
    "        x += self.pos_encoding[:seq_len, :]  \n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e341b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=1000\n",
    "d_model=512\n",
    "\n",
    "embedding_layer = PositionalEmbedding(vocab_size,d_model)\n",
    "random_input = np.random.randint(0,vocab_size,size=(1,100))\n",
    "\n",
    "output= embedding_layer(random_input)\n",
    "print(\"Random imput shape : \", random_input.shape)\n",
    "print(\"PositionalEmbedding output : \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,**kwargs:dict):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
    "        attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)  \n",
    "        x = self.add([x, attn_output])  # Residual connection\n",
    "        x = self.layernorm(x)  # Normalization for stability\n",
    "        self.last_attn_scores = attn_scores  # Store attention scores\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9353eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = 1000\n",
    "decoder_vocab_size = 1000\n",
    "d_model = 768\n",
    "\n",
    "# Fixed vocab_size reference for embeddings\n",
    "encoder_embedding_layer = PositionalEmbedding(encoder_vocab_size, d_model)\n",
    "decoder_embedding_layer = PositionalEmbedding(decoder_vocab_size, d_model)\n",
    "\n",
    "# Random input generation\n",
    "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
    "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
    "\n",
    "# Compute embeddings\n",
    "encoder_embedding = encoder_embedding_layer(random_encoder_input)\n",
    "decoder_embedding = decoder_embedding_layer(random_decoder_input)\n",
    "\n",
    "# Debugging outputs\n",
    "print(f\" Encoder Embedding Shape: {encoder_embedding.shape}\")\n",
    "print(f\" Decoder Embedding Shape: {decoder_embedding.shape}\")\n",
    "\n",
    "# Initialize CrossAttention Layer\n",
    "cross_attention_layer = CrossAttention(num_heads=2, key_dim=d_model)  # Explicitly linking key_dim to d_model\n",
    "cross_attention_output = cross_attention_layer(decoder_embedding, encoder_embedding)\n",
    "\n",
    "print(f\"Cross Attention Output Shape: {cross_attention_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10151033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def __init__(self, **kwargs: dict):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)  #  Added dropout for regularization\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        # Optionally return attention scores for debugging\n",
    "        attn_outpt, attn_scores = self.mha(query=x, key=x, value=x, return_attention_scores=True)\n",
    "\n",
    "        # Apply dropout before residual connection\n",
    "        attn_outpt = self.dropout(attn_outpt, training=training)\n",
    "\n",
    "        # Residual connection and layer normalization\n",
    "        x = self.add([x, attn_outpt])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        # Store attention scores for analysis (Optional)\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = 1000\n",
    "d_model = 768\n",
    "\n",
    "# Fix: Use encoder_vocab_size instead of undefined vocab_size\n",
    "encoder_embedding_layer = PositionalEmbedding(encoder_vocab_size, d_model)\n",
    "\n",
    "# Generate random input\n",
    "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
    "\n",
    "# Pass input through embedding layer\n",
    "encoder_embedding = encoder_embedding_layer(random_encoder_input)\n",
    "\n",
    "print(\"Encoder_Embedding Shape:\", encoder_embedding.shape)\n",
    "\n",
    "# Ensure GlobalSelfAttention has correct argument names\n",
    "cross_attention_layer = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "# Explicitly set training flag for dropout consistency\n",
    "cross_attention_output = cross_attention_layer(encoder_embedding, training=True)\n",
    "\n",
    "print(\"Global_Self_Attention_output shape:\", cross_attention_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcafbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\n",
    "        attn_outpt = self.mha(query=x, key=x, value=x, use_causal_mask=True, training=training)  #  Fix: Explicit training\n",
    "\n",
    "        x = self.add([x, attn_outpt])  # Residual connection\n",
    "        x = self.layernorm(x)  #  Layer normalization for stability\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff68588",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_vocab_size = 1000  #  Defined correctly\n",
    "\n",
    "d_model = 768\n",
    "\n",
    "# Fixed vocab_size variable name\n",
    "decoder_embedding_layer = PositionalEmbedding(decoder_vocab_size, d_model)\n",
    "\n",
    "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
    "\n",
    "decoder_embeddings = decoder_embedding_layer(random_decoder_input)\n",
    "\n",
    "print(\"Decoder_Embeddings shape:\", decoder_embeddings.shape)\n",
    "\n",
    "# Pass training=True explicitly for dropout stability\n",
    "causal_self_attention_layer = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "causal_self_attention_output = causal_self_attention_layer(decoder_embeddings, training=True)  # Explicit training arg\n",
    "\n",
    "print(\"causal_self_attention_output shape:\", causal_self_attention_output.shape)\n",
    "\n",
    "# Slice before embedding layer to prevent shape mismatch\n",
    "out1 = causal_self_attention_layer(decoder_embedding_layer(random_decoder_input[:, :50]), training=False)\n",
    "out2 = causal_self_attention_layer(decoder_embedding_layer(random_decoder_input[:, :50]), training=False)  # Fixed slicing\n",
    "\n",
    "# Compute numerical difference to check consistency\n",
    "diff = tf.reduce_max(tf.abs(out1 - out2)).numpy()\n",
    "\n",
    "print(\"Difference between the two outputs:\", diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b89b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        \n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)  #  Normalize before FFN\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation=tf.keras.layers.ReLU()),  #  More robust activation\n",
    "            tf.keras.layers.Dropout(dropout_rate),  # Dropout for regularization\n",
    "            tf.keras.layers.Dense(d_model)  # Projects back to d_model\n",
    "        ])\n",
    "        self.add_layer = tf.keras.layers.Add()  # Explicit Add() layer\n",
    "\n",
    "    def call(self, x: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        norm_x = self.layer_norm(x)  \n",
    "        seq_out = self.seq(norm_x, training=training)  \n",
    "        x = self.add_layer([x, seq_out])  \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b545543",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = 1000\n",
    "d_model = 768\n",
    "\n",
    "# Fix: Use the correct variable name (encoder_vocab_size)\n",
    "encoder_embedding_layer = PositionalEmbedding(encoder_vocab_size, d_model)\n",
    "\n",
    "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
    "\n",
    "print(\"Encoder_Embeddings shape:\", encoder_embeddings.shape)\n",
    "\n",
    "# Ensure FeedForward runs correctly\n",
    "ff_layer = FeedForward(d_model, dff=2048)\n",
    "ff_output = ff_layer(encoder_embeddings, training=True)  # Fix: Pass training=True\n",
    "\n",
    "print(\"Feed_Forward_Output shape:\", ff_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate  # Ensure dropout is set\n",
    "        )\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)  #  Fix: Add LayerNorm\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)  #  Fix: Second LayerNorm\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)  #  Fix: Add explicit Dropout\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = True) -> tf.Tensor:\n",
    "        #  Apply Self-Attention + Add + Normalize\n",
    "        attn_output = self.self_attention(x)  \n",
    "        x = self.layernorm1(x + attn_output)  \n",
    "\n",
    "        #  Apply FeedForward + Dropout + Normalize\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)  #  Apply dropout only in training\n",
    "        x = self.layernorm2(x + ffn_output)  \n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = 1000\n",
    "d_model = 768\n",
    "\n",
    "# Fix: Use `encoder_vocab_size`\n",
    "encoder_embedding_layer = PositionalEmbedding(encoder_vocab_size, d_model)  \n",
    "\n",
    "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
    "\n",
    "print(\"Encoder_Embeddings shape:\", encoder_embeddings.shape)\n",
    "\n",
    "#  Using the updated EncoderLayer with LayerNorm & Dropout\n",
    "encoder_layer = EncoderLayer(d_model, num_heads=2, dff=2048)\n",
    "\n",
    "encoder_layer_output = encoder_layer(encoder_embeddings, training=True)  #  Ensure training=True during training\n",
    "\n",
    "print(\"Encoder_layer_Output shape:\", encoder_layer_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #  Positional Embedding Layer\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        #  Encoder Layers (List Comprehension)\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)  #  Changed 'a' to '_' (convention for unused variable)\n",
    "        ]\n",
    "\n",
    "        #  Dropout Layer\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        \"\"\" \n",
    "        Forward pass of the Transformer Encoder\n",
    "        - `training` flag ensures dropout is only active during training\n",
    "        \"\"\"\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x, training=training)  #  Ensure dropout applies only during training\n",
    "\n",
    "        #  Iterate through all encoder layers\n",
    "        for i, layer in enumerate(self.enc_layers):\n",
    "            x = layer(x)  # No need to index explicitly (cleaner)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7807b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = 1000  # Ensure the correct variable name\n",
    "d_model = 768\n",
    "\n",
    "#  Pass the correct variable name\n",
    "encoder = Encoder(num_layers=2, d_model=d_model, num_heads=2, dff=2048, vocab_size=encoder_vocab_size)\n",
    "\n",
    "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
    "\n",
    "encoder_output = encoder(random_encoder_input)\n",
    "\n",
    "print(\"Random_Encoder_input shape : \", random_encoder_input.shape)\n",
    "print(\"Encoder_Output shape : \", encoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        #  Causal Self-Attention: Ensures autoregressive behavior\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        #  Cross-Attention: Helps the decoder attend to encoder outputs\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        #  FeedForward Network for feature extraction\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
    "        x = self.causal_self_attention(x=x)  #  Self-attention first\n",
    "        x = self.cross_attention(x=x, context=context)  #  Remove return_attention_scores\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores  #  Store attention scores\n",
    "        x = self.ffn(x)  #  Apply FeedForward\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (Ensure consistency)\n",
    "decoder_vocab_size = 1000\n",
    "d_model = 768\n",
    "dff = 2048\n",
    "num_heads = 8  #  Ensure this matches Encoder's num_heads (previously it was 2!)\n",
    "\n",
    "# Instantiate Decoder Layer\n",
    "decoder_layer = DecoderLayer(d_model, num_heads, dff)\n",
    "\n",
    "#  Generate random input for Decoder\n",
    "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
    "\n",
    "#  Use `decoder_embedding_layer` (not `encoder_embedding_layer`)\n",
    "decoder_embeddings = decoder_embedding_layer(random_decoder_input)\n",
    "\n",
    "#  Ensure `encoder_output` is passed as `context`\n",
    "decoder_layer_output = decoder_layer(decoder_embeddings, context=encoder_output)\n",
    "\n",
    "#  Print Output Shapes for Debugging\n",
    "print(\"Random Decoder Input Shape:\", random_decoder_input.shape)\n",
    "print(\"Decoder Embeddings Shape:\", decoder_embeddings.shape)\n",
    "print(\"Decoder Output Shape:\", decoder_layer_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers  #  Keeping only necessary attributes\n",
    "\n",
    "        #  Positional Embedding & Dropout\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        #  Decoder Layers\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)  \n",
    "        ]\n",
    "\n",
    "        #  Last Attention Scores (Default: None)\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #  Apply Decoder Layers\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, context)\n",
    "\n",
    "        # Store Last Attention Scores (Only if layers exist)\n",
    "        if self.num_layers > 0:\n",
    "            self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define Vocab Size & Model Dimensions\n",
    "decoder_vocab_size = 1000\n",
    "d_model = 768\n",
    "\n",
    "#  Ensure `encoder_output` is defined correctly\n",
    "assert 'encoder_output' in locals(), \"Error: `encoder_output` is not defined. Run the encoder first.\"\n",
    "\n",
    "#  Create Decoder with Better Head Count (8 instead of 2 for `d_model=512`)\n",
    "decoder_layer = Decoder(num_layers=2, d_model=d_model, num_heads=8, dff=2048, vocab_size=decoder_vocab_size)\n",
    "\n",
    "#  Generate Random Decoder Input\n",
    "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 100))\n",
    "\n",
    "#  Pass Inputs to Decoder\n",
    "decoder_output = decoder_layer(random_decoder_input, encoder_output)\n",
    "\n",
    "#  Check for NaN Outputs (Debugging Step)\n",
    "if tf.math.reduce_any(tf.math.is_nan(decoder_output)):\n",
    "    print(\"Warning: NaN values detected in `decoder_output`!\")\n",
    "\n",
    "#  Print Output Shapes\n",
    "print(\"Random_decoder_input shape:\", random_decoder_input.shape)\n",
    "print(\"Decoder_Output shape:\", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b379de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer(\n",
    "    input_vocab_size: int,\n",
    "    target_vocab_size: int,\n",
    "    encoder_input_size: int = None,   # Allow None for dynamic shape\n",
    "    decoder_input_size: int = None,   # Allow None for dynamic shape\n",
    "    num_layers: int = 6,\n",
    "    d_model: int = 512,\n",
    "    num_heads: int = 8,\n",
    "    dff: int = 2048,\n",
    "    dropout_rate: float = 0.1\n",
    ") -> tf.keras.Model:\n",
    "    \n",
    "    #  Define Inputs (None allows variable-length sequences)\n",
    "    encoder_input = tf.keras.Input(shape=(encoder_input_size or None,), dtype=tf.int64, name=\"encoder_input\")\n",
    "    decoder_input = tf.keras.Input(shape=(decoder_input_size or None,), dtype=tf.int64, name=\"decoder_input\")\n",
    "    \n",
    "    #  Build Encoder & Decoder\n",
    "    encoder = Encoder(\n",
    "        num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "        dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate\n",
    "    )(encoder_input)\n",
    "\n",
    "    decoder = Decoder(\n",
    "        num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "        dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate\n",
    "    )(decoder_input, encoder)\n",
    "\n",
    "    #  Add Final Dense Layer with `softmax` Activation\n",
    "    output = tf.keras.layers.Dense(target_vocab_size, activation=\"softmax\", name=\"output_layer\")(decoder)\n",
    "\n",
    "    #  Define & Return Model\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=output, name=\"Transformer_Model\")\n",
    "    \n",
    "    #  Print Model Summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc24d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_size=100\n",
    "decoder_input_size=110\n",
    "\n",
    "encoder_vocab_size=1000\n",
    "decoder_vocab_size=1000\n",
    "\n",
    "model=Transformer(\n",
    "    input_vocab_size=encoder_vocab_size,\n",
    "    target_vocab_size=decoder_vocab_size,\n",
    "    encoder_input_size=encoder_input_size,\n",
    "    decoder_input_size=decoder_input_size,\n",
    "    num_layers=2,\n",
    "    d_model=512,\n",
    "    num_heads=2,\n",
    "    dff=512,\n",
    "    dropout_rate=0.1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "#  Optimized Learning Rate & Dropout\n",
    "learning_rate_schedule = CosineDecayRestarts(\n",
    "    initial_learning_rate=0.0003, first_decay_steps=1000, t_mul=2.0\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "dropout_rate = 0.1  # Best-performing dropout\n",
    "\n",
    "#  Load Pretrained GloVe 100D (Best Performing)\n",
    "embedding_dim = 50  \n",
    "start_time = time.time()\n",
    "glove_model = api.load(\"glove-twitter-50\")  \n",
    "print(f\" Pretrained embeddings loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#  Embed Words Using GloVe 100D\n",
    "vocab_size = 10000\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = glove_model.get_vector(word) if word in glove_model else np.random.uniform(-0.1, 0.1, embedding_dim)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True\n",
    ")\n",
    "\n",
    "#  Improved Beam Search (Finalized)\n",
    "def beam_search_decoder(predictions, beam_width=10, alpha=0.5):  \n",
    "    sequences = [[list(), 1.0]]\n",
    "    for row in predictions:\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            for j, prob in enumerate(row):\n",
    "                prob = max(prob, 1e-9)  \n",
    "                length_norm = (1 + len(seq))*alpha / (1 + 1)*alpha  \n",
    "                candidate = [seq + [j], score + (-np.log(prob) / length_norm)]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_width]\n",
    "\n",
    "    return sequences[0][0]\n",
    "\n",
    "#  Optimized Synonym Replacement\n",
    "def synonym_replace(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 3: return sentence  # Avoid modifying very short sentences\n",
    "    word_idx = random.randint(0, len(words) - 1)\n",
    "    synonyms = [lemma.name() for syn in wordnet.synsets(words[word_idx]) for lemma in syn.lemmas()]\n",
    "    synonyms = list(set(synonyms) - {words[word_idx]})  \n",
    "    if synonyms:\n",
    "        words[word_idx] = random.choice(synonyms)\n",
    "    return \" \".join(words)\n",
    "\n",
    "#  Fix Cosine Similarity Calculation\n",
    "def get_sentence_vector(sentence):\n",
    "    vectors = [embedding_matrix[word_index.get(word, 0)] for word in sentence]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(embedding_dim)\n",
    "\n",
    "#  Model Evaluation (Best Version)\n",
    "def evaluate_model(y_true, y_pred_logits, idx2word):\n",
    "    y_pred_classes = np.argmax(y_pred_logits, axis=-1)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "    loss_values = [loss_fn(tf.one_hot(y_true[i], depth=len(idx2word)), y_pred_logits[i]).numpy() for i in range(len(y_true))]\n",
    "    avg_loss = np.mean(loss_values)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    ref_sentences = [[idx2word.get(idx, \"<UNK>\") for idx in y_true[i] if idx > 0] for i in range(len(y_true))]\n",
    "    pred_sentences = [[idx2word.get(idx, \"<UNK>\") for idx in y_pred_classes[i] if idx > 0] for i in range(len(y_pred_classes))]\n",
    "\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = [sentence_bleu([ref], pred, smoothing_function=smoothing) for ref, pred in zip(ref_sentences, pred_sentences)]\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = [scorer.score(\" \".join(ref), \" \".join(pred)) for ref, pred in zip(ref_sentences, pred_sentences)]\n",
    "    avg_rouge1 = np.mean([s['rouge1'].fmeasure for s in rouge_scores])\n",
    "    avg_rouge2 = np.mean([s['rouge2'].fmeasure for s in rouge_scores])\n",
    "    avg_rougeL = np.mean([s['rougeL'].fmeasure for s in rouge_scores])\n",
    "\n",
    "    cosine_similarities = [cosine_similarity([get_sentence_vector(ref)], [get_sentence_vector(pred)])[0, 0] for ref, pred in zip(ref_sentences, pred_sentences)]\n",
    "    avg_cosine = np.mean(cosine_similarities)\n",
    "    \n",
    "    print(f\" Model Perplexity: {perplexity:.4f}\")\n",
    "    print(f\" Average Log-Likelihood (Loss): {avg_loss:.4f}\")\n",
    "    print(f\" BLEU Score: {avg_bleu:.4f}\")\n",
    "    print(f\" ROUGE Scores - ROUGE-1: {avg_rouge1:.4f}, ROUGE-2: {avg_rouge2:.4f}, ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    print(f\" Cosine Similarity: {avg_cosine:.4f}\")\n",
    "\n",
    "#  Running Final Evaluation\n",
    "batch_size = 64  \n",
    "dummy_input = np.random.randint(0, 100, size=(1000, 10))  \n",
    "dummy_output = np.random.randint(0, 100, size=(1000, 10))  \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dummy_input, dummy_output))\n",
    "dataset = dataset.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "y_true = np.random.randint(1, 6, size=(5, 5))\n",
    "y_pred_logits = np.random.rand(5, 5, len(word_index))\n",
    "\n",
    "idx2word = {i: w for w, i in word_index.items()}\n",
    "\n",
    "evaluate_model(y_true, y_pred_logits, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a018bd-3b8c-438a-8df6-5f302f1cab71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
